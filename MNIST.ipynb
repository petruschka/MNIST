{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project deals with the classification of handwritten digits. MLPs (Multilayer Perceptrons) are trained to predict digits in the MNIST dataset. Especially the dropout technuque is demonstrated to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into the training and testing dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset contains 60,000 images, while the testing dataset contains 10,000 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the first 10 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(1, 5, i+1)\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encodig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In the original dataset numbers are classified using the numbers 0-9\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode training and testing categories\n",
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The encoded dataset contains a vector with 10 binary entries. The location were the entry is \"activated\" \n",
    "#corresponds to the original number.\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model uses an input layer, 2 hidden layers with 100 neurons respectively and an output layer with 10 nodes. The hidden layers use ReLUs as activation functions, while the output layer uses softmax in order to predict probabilities of being in each number category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dropout, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the accuracy with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 93us/step\n",
      "['loss', 'acc']\n",
      "[14.391624537658691, 0.0858]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(x_test, y_test)\n",
    "print(model.metrics_names)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the prediction accuracy is very low with initialized random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 5.9445 - acc: 0.6203 - val_loss: 5.0049 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.00493, saving model to mnist.model.best.hdf5\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 4.5226 - acc: 0.7137 - val_loss: 4.4173 - val_acc: 0.7218\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.00493 to 4.41733, saving model to mnist.model.best.hdf5\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 4.3598 - acc: 0.7253 - val_loss: 4.2807 - val_acc: 0.7308\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.41733 to 4.28069, saving model to mnist.model.best.hdf5\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 4.2433 - acc: 0.7337 - val_loss: 4.1534 - val_acc: 0.7398\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.28069 to 4.15340, saving model to mnist.model.best.hdf5\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 4.1968 - acc: 0.7367 - val_loss: 4.0796 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.15340 to 4.07962, saving model to mnist.model.best.hdf5\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 4.2276 - acc: 0.7352 - val_loss: 4.1601 - val_acc: 0.7403\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.07962\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 4.1052 - acc: 0.7434 - val_loss: 4.2057 - val_acc: 0.7368\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.07962\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 4.0827 - acc: 0.7447 - val_loss: 4.0250 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.07962 to 4.02499, saving model to mnist.model.best.hdf5\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 4.0188 - acc: 0.7485 - val_loss: 4.2243 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.02499\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 4.0065 - acc: 0.7496 - val_loss: 4.1660 - val_acc: 0.7398\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.02499\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 3.9682 - acc: 0.7521 - val_loss: 4.1196 - val_acc: 0.7429\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.02499\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 3.9302 - acc: 0.7545 - val_loss: 4.0623 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.02499\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 3.9646 - acc: 0.7525 - val_loss: 4.1525 - val_acc: 0.7413\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.02499\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.9525 - acc: 0.7533 - val_loss: 4.0395 - val_acc: 0.7482\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 4.02499\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 3.9074 - acc: 0.7561 - val_loss: 3.9690 - val_acc: 0.7527\n",
      "\n",
      "Epoch 00015: val_loss improved from 4.02499 to 3.96902, saving model to mnist.model.best.hdf5\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 3.8886 - acc: 0.7575 - val_loss: 3.9440 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.96902 to 3.94401, saving model to mnist.model.best.hdf5\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.9352 - acc: 0.7546 - val_loss: 4.0835 - val_acc: 0.7454\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.94401\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.9090 - acc: 0.7563 - val_loss: 4.1530 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.94401\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.8784 - acc: 0.7584 - val_loss: 4.2569 - val_acc: 0.7342\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.94401\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 3.8534 - acc: 0.7598 - val_loss: 3.9254 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.94401 to 3.92540, saving model to mnist.model.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "   \n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "hist = model.fit(x_train, y_train, batch_size=128, epochs=20,\n",
    "          validation_split=0.2, callbacks=[checkpointer],\n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('mnist.model.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 56us/step\n",
      "['loss', 'acc']\n",
      "[3.878389277267456, 0.7581]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(x_test, y_test)\n",
    "print(model.metrics_names)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the accuracy increases dramatically (around 76%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP Model with dropout technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model adds two additional layer after each hidden layer. These dropout layers help reduce overfitting using a dropout rate of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dropout, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 6.6367 - acc: 0.5749 - val_loss: 3.6281 - val_acc: 0.7665\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.62812, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 3.6793 - acc: 0.7623 - val_loss: 2.8415 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.62812 to 2.84152, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 2.6781 - acc: 0.8264 - val_loss: 1.3859 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.84152 to 1.38591, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 2.0101 - acc: 0.8685 - val_loss: 1.4495 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.38591\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.8195 - acc: 0.8813 - val_loss: 1.3653 - val_acc: 0.9119\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.38591 to 1.36532, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.7012 - acc: 0.8892 - val_loss: 1.1053 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.36532 to 1.10533, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.6253 - acc: 0.8946 - val_loss: 1.1871 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.10533\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.5060 - acc: 0.9028 - val_loss: 1.0643 - val_acc: 0.9312\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.10533 to 1.06427, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.4030 - acc: 0.9092 - val_loss: 0.9564 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.06427 to 0.95639, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 1.4137 - acc: 0.9086 - val_loss: 1.0073 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.95639\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.4116 - acc: 0.9093 - val_loss: 1.1308 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.95639\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 1.3441 - acc: 0.9137 - val_loss: 1.0031 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.95639\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.3033 - acc: 0.9159 - val_loss: 0.9206 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.95639 to 0.92056, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 1.2962 - acc: 0.9164 - val_loss: 0.9242 - val_acc: 0.9413\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.92056\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.2553 - acc: 0.9192 - val_loss: 0.9375 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.92056\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.2572 - acc: 0.9194 - val_loss: 0.9840 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.92056\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 1.2701 - acc: 0.9184 - val_loss: 0.8926 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.92056 to 0.89257, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.2369 - acc: 0.9211 - val_loss: 0.8770 - val_acc: 0.9443\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.89257 to 0.87703, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.1680 - acc: 0.9252 - val_loss: 0.8393 - val_acc: 0.9463\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.87703 to 0.83934, saving model to mnist_dropout.model.best.hdf5\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.1185 - acc: 0.9282 - val_loss: 0.8250 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.83934 to 0.82501, saving model to mnist_dropout.model.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "   \n",
    "checkpointer = ModelCheckpoint(filepath='mnist_dropout.model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "hist = model.fit(x_train, y_train, batch_size=128, epochs=20,\n",
    "          validation_split=0.2, callbacks=[checkpointer],\n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('mnist_dropout.model.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 55us/step\n",
      "['loss', 'acc']\n",
      "[0.8326474254135557, 0.9474]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(x_test, y_test)\n",
    "print(model.metrics_names)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model generalizes a lot better and thus produces an accuracy of around 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model using the dropout technique performs better in the current iteration, this behaviour is not always observable. In 2 out of 4 iterations the simple model performed actually better (although the performance was only only slightly better by 2-3%). The model using a dropout rate on the other hand seems to perform extremely stable producing a constant accuracy of approximately 95%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
